---
title: "Trying out some data analysis"
date: "20/10/2021"
description: ''
draft: no
image: havingfun.jpg
keywords: ''
slug: fun
categories:
- ''
- ''
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(ggrepel)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```



# Where Do People Drink The Most Beer, Wine And Spirits?


```{r, load_alcohol_data}
library(fivethirtyeight)
data(drinks)


# or download directly
#alcohol_direct <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")

```


What are the variable types? Any missing values we should worry about?  

1) Char, integer, numerical decimal
2) No missing values

```{r glimpse_skim_data}

glimpse(drinks) #Getting an overview of the data
is.na(drinks)  #Checking for na values
which(is.na(drinks)) #Checking for na values 
```


Make a plot that shows the top 25 beer consuming countries

```{r beer_plot}
#Filtering the top 25 beer consuming countries 
top_25 <- drinks %>%
  arrange(desc(beer_servings)) %>%
  head(25)
#Creating a plot for the top 25 beer consuming countries 
ggplot(top_25, aes(y= fct_reorder(country,beer_servings), x=beer_servings, fill=country))+
  geom_col()+
  labs(title ="Top 25 countries by beer consumption", y ="Country", x ="Beer consumption")
  NULL
```

Make a plot that shows the top 25 wine consuming countries

```{r wine_plot}
#Filtering the top 25 wine consuming countries
top_25 <- drinks %>%
  arrange(desc(wine_servings)) %>%
  head(25)
#Creating a plot for the top 25 wine consuming countries 
ggplot(top_25, aes(y= fct_reorder(country,wine_servings), x=wine_servings, fill=country))+
  geom_col()+
  labs(title="Top 25 countries by wine consumption", y="Country",x="Wine consumption")
  NULL

```

Finally, make a plot that shows the top 25 spirit consuming countries
```{r spirit_plot}
#Filtering the top 25 spirit consuming countries 
top_25 <- drinks %>%
  arrange(desc(spirit_servings)) %>%
  head(25)
#Creating a plot for the top 25 spirit consuming countries 
ggplot(top_25, aes(y= fct_reorder(country,spirit_servings), x=spirit_servings, fill=country))+
  geom_col()+
  labs(title="Top 25 countries by spirit consumption", y="Country",x="Spirit consumption")
  NULL
```

These plots highlight that the largest consumers of beer, wine and spirit are usually also the largest producers. One of the world's largest producers of beer is Germany and it features at the 4th spot among the top 25 beer consuming countries. Furthermore France produces the greatest amount of wine and it is also the largest cosumer of wine.

There are certain geographical patterns that can be observed from these graphs as well. The top beer consuming countries are spread across Africa,Europe and South America, but the top wine consuming countries are mostly European. Top spirit consuming countries are developing countries, and no major developed country feautres on the list. This shows that developed countries have a stronger propensity to consume fine liquors such as Wine whereas developing and low to middle income countries usually demostrate a preference for spirit. 

# Analysis of movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)

  
```{r,load_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data", "movies.csv")) #Load the dataset 
glimpse(movies) #Get a summary of the data 

```


```{r}
which(is.na(movies)) #checking for na values
length(unique(movies$title)) #checking if there are non-unique rows 
movies[duplicated(movies$title),] #using the duplicate() fucntion to return a list of duplicate movie titles 
```

- Produce a table with the count of movies by genre, ranked in descending order
```{r}
genre <- movies %>% #assigning table name 
  group_by(genre) %>% #grouping movie data by genre 
  count(genre) %>% #counting the genre 
  arrange(desc(n)) #arranging the count of genre in descending order 

genre #return table as output 
  

```

table with the average gross earning and budget (`gross` and `budget`) by genre.

```{r}
movies %>% #selecting the movies data
  group_by(genre) %>% #grouping the data by genre 
  summarize(average_gross = mean(gross), average_budget = mean(budget)) #calculating average of gross revenue and budget by genre 

movies %>% #selecting the movies data
  group_by(genre) %>% #grouping the data by genre 
  summarize(return_on_budget = mean(gross)/mean(budget)) %>%
  arrange(desc(return_on_budget))


```

table that shows the top 15 directors who have created the highest gross revenue in the box office. 

```{r}
movies %>% #selecting the movies data
  group_by(director) %>% #groupping the data by director 
  summarize(average_gross = mean(gross), median_gross=median(gross),sd_gros=StdDev(gross))%>% #calculating mean, median and standard deviation of the the gross revenue by director 
  arrange(desc(average_gross))%>% #arranfing the data in descending order 
  head(15) #retunrning the top 15 values 

```

table that describes how ratings are distributed by genre. 

```{r}
summary_ratings <- movies %>% #creating a table for summary of ratings 
  group_by(genre) %>% #groupping by genre 
  summarize(average_ratings = mean(rating), median_ratings=median(rating),sd_ratings=StdDev(rating), min_ratings = min(rating), max_ratings=max(rating))%>% # finding the mean, median, standard deviation, min and max of the data 
  arrange(genre)
summary_ratings


p <- ggplot(movies, aes(x = rating )) + geom_density(color="darkblue", fill="lightblue") + 
  labs(title = "Density plot of movie ratings" ) #plotting the density of movies data 
# Add mean line
p+ geom_vline(aes(xintercept=mean(rating)),
            color="blue", linetype="dashed", size=1)

```

There exists a positive relationship between the number of facebook likes and the gross revenue generated by the movie, as depicted by the trend line that summarizes the nature of the scatter plot. We have chosen gross revenue as the Y variable since we are predicting the impact of facebook likes on the revenue generated. Y variable - gross revenue is the dependent variable in this case which depends on the independent variable X - facebook likes. 

```{r, gross_on_fblikes}
ggplot(movies, aes(x = cast_facebook_likes , y = gross )) + #plotting the scatterplot of facebook likes and the gross revenue of the movie 
  geom_point(alpha=0.3) + 
  geom_smooth(method = "lm") + #adding trendline 
  scale_x_log10()+  #scalling the data 
  scale_y_log10() +
  theme_bw() + 
  labs(title = "Relationship between gross revenue and facebook likes", y="Gross revenue", x=" Count of facebook likes" ) + #adding titles and axes labels 
  NULL
```

  - Examining the relationship between `gross` and `budget`.

The scatter plot and the trendline demonstrate that films with higher budget generally have higher revenue as there exists a positive relationship between these variables. 

```{r, gross_on_budget}
ggplot(movies, aes(y= gross , x = budget )) + #plotting the scatterplot of budget and the gross revenue of the movie 
  geom_point(alpha=0.3) + 
  geom_smooth(method = "lm") + 
  scale_x_log10()+ 
  scale_y_log10() +
  theme_bw() +
  labs(title = "Relationship between gross revenue and budget of films", y="Gross revenue", x="Budget") +
  NULL
```
  
  - Examining the relationship between `gross` and `rating`. 
  
The faceted scatter plot depicts that the IMDB ratings are not a very good indicator of the gross revenue that the film will make, since the relationship between these variables is not consistent across genres. While for some genres there exists a positive relationship (drama, action, adventure, etc.) for a few other the relationship is negative (Documentary and Sci-Fi). 

```{r, gross_on_rating}
ggplot(movies, aes(y= gross , x = rating, colour = genre )) + #Facted plot of ratings and revenue
  geom_point(alpha=0.3) + 
  geom_smooth(method = "lm") + 
  scale_x_log10()+ 
  scale_y_log10() +
  theme_bw() +
  labs(title = "Relationship between gross revenue and rating of films, grouped by genre" ) +
  facet_wrap(~genre) + 
  NULL
```


# Returns of financial stocks

We will use the `tidyquant` package to download historical data of stock prices, calculate returns, and examine the distribution of returns.

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv")) #load the dataset
glimpse(nyse) #checking summary of data 
is.na(nyse) #checking for na values 
```

We create a table and a bar plot that shows the number of companies per sector, in descending order

```{r companies_per_sector}

# YOUR CODE GOES HERE
table1 <- nyse %>% #creating table variable 
  count(sector) %>% #counting companies by sector 
  arrange(desc(n)) #arranging in descending order 
table1

ggplot(table1, aes(x= n , y = fct_reorder(sector, n), fill = sector)) + #generating plot 
  geom_col(show.legend = FALSE) + 
  labs(title = "Number of companies per sector" , y = "sector", x = "number of companies") +
  NULL
```

```{r, tickers_from_wikipedia}

djia_url <- "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"


#get tables that exist on URL
tables <- djia_url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia <- map(tables, . %>% 
               html_table(fill=TRUE)%>% 
               clean_names())


# constituents
table1 <- djia[[2]] %>% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains "NYSE*", the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, "NYSE*"),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers <- table1 %>% 
  select(ticker) %>% 
  pull() %>% # pull() gets them as a sting of characters
  c("SPY") # and lets us add SPY, the SP500 ETF

```

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- tickers %>% 
  tq_get(get  = "stock.prices",
         from = "2000-01-01",
         to   = Sys.Date()) %>% # Sys.Date() returns today's price
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.


```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

We create a table where you summarise monthly returns for each of the stocks and `SPY`; min, max, median, mean, SD.

```{r summarise_monthly_returns}

# YOUR CODE GOES HERE
summary_returns <- myStocks_returns_monthly %>% #Creating a new table for summarizing data 
  group_by(symbol) %>% #grouping data by stock based 
  summarize(average_returns = mean(monthly_returns), median_returns=median(monthly_returns),sd_returns=StdDev(monthly_returns), min_returns = min(monthly_returns), max_returns=max(monthly_returns))%>% 
  arrange(desc(sd_returns)) #arranging the data in desceding order of standard deviation
summary_returns

summary_returns %>% #filtering results for SPY 
filter(symbol == "SPY") 

```


We Plot a density plot, using `geom_density()`, for each of the stocks
```{r density_monthly_returns}

# YOUR CODE GOES HERE
p2<- ggplot(myStocks_returns_monthly, aes(x = monthly_returns )) +  #assigning new variable for the plot 
  geom_density(color="darkblue", fill="lightblue") + 
  facet_wrap(~symbol) + 
  labs(title = "Distribution of monthly returns, grouped by stocks" , x = "Monthly returns")
# Add mean line
p2 + geom_vline(aes(xintercept=mean(monthly_returns)),
            color="blue", linetype="dashed", size=1)
```

The more the returns are spread out, the riskier is the stocks. This is confirmed by the information in the table: the riskiest is Apple (AAPL) and the least risky is Johnson & Johnson (JNJ). 

Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock

```{r risk_return_plot}
# YOUR CODE GOES HERE
ggplot(summary_returns, aes(x = sd_returns, y = average_returns, label = symbol)) + 
  geom_point() +
  geom_text_repel() + labs(title = "Monthly returns to risk", y="expected monthly returns", x="standard deviation of return")+ #using geom_text_repel
  NULL


```

Higher standard deviation of return conveys the higher risk assumed for acheiving expected returns. The higher the risk assumed (higher standard deviation) the higher are the returns (higher expected monthly retuns).


# Is inflation transitory?

> The surge in inflation seen across major economies is probably short lived because it’s confined to just a few sectors of the economy, according to the Bank for International Settlements. 

> New research by the BIS’s Claudio Borio, Piti Disyatat, Egon Zakrajsek and Dora Xia adds to one of the hottest debates in economics -- how long the current surge in consumer prices will last. Both Federal Reserve Chair Jerome Powell and his euro-area counterpart Christine Lagarde have said the pickup is probably transitory, despite a snarled global supply chain and a spike in energy prices. 

```{r cpi_10year, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "cpi_10year.png"), error = FALSE)
```

```{r, get_cpi_10Year_yield}

cpi  <-   tq_get("CPIAUCSL", get = "economic.data",
                       from = "1980-01-01") %>% 
  rename(cpi = symbol,  # FRED data is given as 'symbol' and 'price'
         rate = price) %>% # we rename them to what they really are, e.g., cpi and rate
  
  # calculate yearly change in CPI by dividing current month by same month a year (or 12 months) earlier, minus 1
  mutate(cpi_yoy_change = rate/lag(rate, 12) - 1)

ten_year_monthly  <-   tq_get("GS10", get = "economic.data",
                       from = "1980-01-01") %>% 
  rename(ten_year = symbol,
         yield = price) %>% 
  mutate(yield = yield / 100) # original data is not given as, e.g., 0.05, but rather 5, for five percent

# we have the two dataframes-- we now need to join them, and we will use left_join()
# base R has a function merge() that does the same, but it's slow, so please don't use it

mydata <- 
  cpi %>% 
  left_join(ten_year_monthly, by="date") %>% 
  mutate(
    year = year(date), # using lubridate::year() to generate a new column with just the year
    month = month(date, label = TRUE),
    decade=case_when(
      year %in% 1980:1989 ~ "1980s",
      year %in% 1990:1999 ~ "1990s",
      year %in% 2000:2009 ~ "2000s",
      year %in% 2010:2019 ~ "2010s",
      TRUE ~ "2020s"
      )
  )
```

```{r,fig.width = 17, fig.height = 10}
```


```{r,fig.width = 17, fig.height = 10}
glimpse(mydata)
```


```{r,fig.width = 17, fig.height = 10}
ggplot(mydata, aes(y= yield, x = cpi_yoy_change, colour=decade, label = date)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_grid(vars(rows = decade)) + 
  labs(title = "How are CPI and 10 year yield related", y = "10-Year Treasury Constant Maturity Rate", x= "CPI Yearly change")
```





# Challenge 2: Opinion polls for the 2021 German elections


```{r, scrape_wikipedia_polling_data, warnings= FALSE, message=FALSE}
url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election"

# similar graphs and analyses can be found at 
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel


# get tables that exist on wikipedia page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of opinion polls
german_election_polls <- polls[[1]] %>% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %>%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )
 

```

```{r}
glimpse(german_election_polls) #summary of data 
head(german_election_polls)
```
```{r}
my_colour_palette = c(
  "#000000", #CDU
  "#E3000F", #SPD
  "#1AA037", #GRUNE
  "#FFEF00", #FDP
  "#0489DB", #AFD
  "#951d7a"  #LINKE-- we need the pink hex code for Die Linke
)

data_polls <- data.frame(x = german_election_polls$end_date, # Reshape data frame
                       y = c(german_election_polls$union, german_election_polls$spd, german_election_polls$grune, german_election_polls$fdp, german_election_polls$af_d, german_election_polls$linke ),
                       group = c(rep("CDU/CSU", nrow(german_election_polls)),
                                 rep("SPD", nrow(german_election_polls)),
                                 rep("GRUNE", nrow(german_election_polls)),
                                 rep("FDP", nrow(german_election_polls)),
                                 rep("AFD", nrow(german_election_polls)),
                                 rep("LINKE", nrow(german_election_polls))))

ggplot(data_polls, aes(x, y, col = group)) +  # Create ggplot2 plot
  geom_point(alpha=0.3) + 
  geom_smooth(se=FALSE) + 
  theme_bw() + 
  labs(title = "German elections poll tracker", x = "Date", y = "%") + 
  scale_colour_manual(values = my_colour_palette)

```










